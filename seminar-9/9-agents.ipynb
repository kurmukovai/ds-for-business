{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Agents\n",
        "\n",
        "Previously, we utilized large language models (LLMs) to solve various natural language problems. While LLMs are proficient in understanding natural language, they can make mistakes when confronted with more complex tasks. For instance, they may generate code without the ability to verify its functionality since they lack the capability to execute it. Another limitation is that these models are predominantly trained on data available before 2021, meaning they lack knowledge of current real-world events. To address this, it is necessary to provide contextual information to help them access more recent events.\n",
        "\n",
        "One approach to achieving this is by enabling the model to perform certain actions, such as executing code, searching the internet, and utilizing other tools. The various tools are outlined in greater detail [here](https://python.langchain.com/en/latest/modules/agents/tools.html).\n",
        "\n",
        "In the past, we employed a somewhat similar technique by combining nearest neighbors search with further rewriting answers by the LLM. However, instead of using predefined chains of tools, we now empower the LLM itself to determine which action to take in a given situation.\n",
        "\n",
        "The second important component of the agents is the `reasoning` or `chain-of-thought` prompting (or similar).\n",
        "\n",
        "![ReAct](https://react-lm.github.io/files/diagram.png)\n",
        "\n",
        "\n",
        "Below is the example of such an approach\n",
        "\n",
        "![ReAct Example](https://react-lm.github.io/files/hotpotqa.png)"
      ],
      "metadata": {
        "id": "jy069fnqNycC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai langchain duckduckgo-search tiktoken faiss-cpu"
      ],
      "metadata": {
        "id": "ED5E6z0fygai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "print(llm(\"What model was presented by the OpenAI in November 2022?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YexQau915q45",
        "outputId": "e404cca0-84d7-4320-afff-a85814403279"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:171: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:740: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI language model, I cannot predict future events or developments. However, as of now (September 2021), there has been no news or announcement from OpenAI regarding a model to be presented in November 2022.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"ddg-search\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "agent.run(\"What model was presented by the OpenAI in November 2022?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "d_TI5PhCsdIJ",
        "outputId": "888d13f3-2557-4564-d3c0-5f009dab3a5a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to search for recent news articles about OpenAI's latest model.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"OpenAI latest model November 2022\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIllustration: Ruby Chen November 30, 2022 Authors OpenAI Product, Announcements ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users' feedback and learn about its strengths and weaknesses. ChatGPT is an artificial intelligence (AI) chatbot developed by OpenAI and released in November 2022. The name \"ChatGPT\" combines \"Chat\", referring to its chatbot functionality, and \"GPT\", which stands for Generative Pre-trained Transformer, a type of large language model (LLM). [2] Artificial intelligence ChatGPT is OpenAI's latest fix for GPT-3. It's slick but still spews nonsense The new version of the company's large language model makes stuff up—but can also admit... DALL·E 2 is an AI system that can create realistic images and art from a description in natural language. Try DALL·E Follow on Instagram DALL·E 2 explained 2:47 Latest updates View all updates DALL·E API now available in public beta Nov 3, 2022 DALL·E 2: Extending creativity Jul 14, 2022 DALL·E now available without waitlist Sep 28, 2022 Announcing OpenAI's Bug Bounty Program. Apr 11, 2023 April 11, 2023. ... New and improved embedding model. Dec 15, 2022 December 15, 2022. Discovering the minutiae of backend systems. Dec 8, 2022 December 8, 2022. Introducing ChatGPT. Nov 30, 2022 November 30, 2022. DALL·E API now available in public beta. Nov 3, 2022 November 3, ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI found the information I was looking for.\n",
            "Thought: I need to provide the answer to the original question.\n",
            "Final Answer: OpenAI presented ChatGPT in November 2022.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OpenAI presented ChatGPT in November 2022.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How it looks inside?\n",
        "\n",
        "It is just a sophisticated prompt for LLM\n",
        "\n",
        "```json\n",
        "\"\"\"\n",
        "Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "bwigHj6edg7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"What model was presented by the OpenAI in November 2022? How many tokens the model can keep in the context?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "vjkl2VD1_w0N",
        "outputId": "39271bc9-07fd-4c10-c228-fcc6093f17db"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to search for recent news about OpenAI's models.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"OpenAI model November 2022\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNovember 30, 2022 Authors OpenAI Product, Announcements ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users' feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. ChatGPT is an artificial intelligence (AI) chatbot developed by OpenAI and released in November 2022. The name \"ChatGPT\" combines \"Chat\", referring to its chatbot functionality, and \"GPT\", which stands for Generative Pre-trained Transformer, a type of large language model (LLM). [2] Will Douglas Heaven November 30, 2022 Stephanie Arnett/MITTR Buzz around GPT-4, the anticipated but as-yet-unannounced follow-up to OpenAI's groundbreaking large language model, GPT-3, is... OpenAI's ChatGPT was launched in November 2022 and took the world by storm, gaining a million+ users within days - and 100 million users within two months. Then in late January 2023, when Microsoft announced it had invested $10 billion in the tech, the buzz intensified. Announcing OpenAI's Bug Bounty Program. Apr 11, 2023 April 11, 2023. Our approach to AI safety. ... New and improved embedding model. Dec 15, 2022 December 15, 2022. ... Dec 8, 2022 December 8, 2022. Introducing ChatGPT. Nov 30, 2022 November 30, 2022. DALL·E API now available in public beta. Nov 3, 2022 November 3, 2022. 1 - 20 of 81 ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to refine my search to find information about the token capacity of ChatGPT.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"ChatGPT token capacity\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe following sections provide you with a quick guide to the quotas and limits that apply to the Azure OpenAI: Limit Name. Limit Value. OpenAI resources per region per Azure subscription. 3. Requests per minute per model*. Davinci-models (002 and later): 120. ChatGPT model: 300. GPT-4 models: 18. Published May 3, 2023 Tokens help you control how ChatGPT responds. Here's what they are and how to use them. Readers like you help support MUO. When you make a purchase using links on our site, we may earn an affiliate commission. Read More. ChatGPT has taken the technological world by storm, and there is no shortage of advancements and updates. The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. ChatGPT Token Limit Ask Question Asked 3 months ago Modified 5 days ago Viewed 14k times 13 I want ChatGPT to remember past conversations and have a consistent (stateful) conversation. I have seen several code of ChatGPT prompt engineering. There were two ways to design the prompt shown below (pseudo code): 1 The_Lovely_Blue_Faux • 2 mo. ago Text gets coded into token. It says 1000 tokens is around 750 words. That is what they are talking about for input and output size. I think how much GPT remembers from the conversation is just from a summary that they make themselves, like a scratch pad.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to calculate how many words 1000 tokens can hold.\n",
            "Action: Calculator\n",
            "Action Input: 1000 tokens / 750 words\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.3333333333333333\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to round up the answer to a whole number.\n",
            "Action: Calculator\n",
            "Action Input: ceil(1.3333333333333333)\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe ChatGPT model can keep approximately 2 words per token in its context.\n",
            "Final Answer: The ChatGPT model can keep approximately 2 words per token in its context.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The ChatGPT model can keep approximately 2 words per token in its context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It gives us the wrong answer. To improve the model's performance, we can introduce an additional planning step when the model starts, by decomposing the initial query into several steps to follow."
      ],
      "metadata": {
        "id": "darGT6emL9fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
        "\n",
        "planner = load_chat_planner(llm)\n",
        "executor = load_agent_executor(llm, tools, verbose=True)\n",
        "planning_agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)\n",
        "\n",
        "planning_agent.run(\"What model was presented by the OpenAI in November 2022? How many tokens the model can keep in the context?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6wwiSsYAEgbk",
        "outputId": "604a33d5-3440-4f16-a0d9-cae3a3ca60fa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n",
            "steps=[Step(value='Research the OpenAI website and other relevant sources to find information about the model presented in November 2022.'), Step(value='Look for information about the number of tokens the model can keep in the context.'), Step(value='If the information is not readily available, look for articles or papers that discuss the model and its capabilities.'), Step(value='Once the information is found, summarize it in a clear and concise manner.'), Step(value=\"Given the above steps taken, respond to the user's original question.\\n\\n\")]\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"OpenAI model November 2022\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNovember 30, 2022 Authors OpenAI Product, Announcements ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users' feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. ChatGPT is an artificial intelligence (AI) chatbot developed by OpenAI and released in November 2022. The name \"ChatGPT\" combines \"Chat\", referring to its chatbot functionality, and \"GPT\", which stands for Generative Pre-trained Transformer, a type of large language model (LLM). [2] Will Douglas Heaven November 30, 2022 Stephanie Arnett/MITTR Buzz around GPT-4, the anticipated but as-yet-unannounced follow-up to OpenAI's groundbreaking large language model, GPT-3, is... OpenAI's ChatGPT was launched in November 2022 and took the world by storm, gaining a million+ users within days - and 100 million users within two months. Then in late January 2023, when Microsoft announced it had invested $10 billion in the tech, the buzz intensified. How Generative AI Is Changing Creative Work. by. Thomas H. Davenport. and. Nitin Mittal. November 14, 2022. Andriy Onufriyenko/Getty Images. Summary. Generative AI models for businesses threaten ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe human asked for information about the OpenAI model presented in November 2022. The search result provided information about ChatGPT, an AI chatbot developed by OpenAI and released in November 2022. It gained a million+ users within days and 100 million users within two months. Microsoft also announced a $10 billion investment in the technology. \n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It gained a million+ users within days and 100 million users within two months. Microsoft also announced a $10 billion investment in the technology.\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Research the OpenAI website and other relevant sources to find information about the model presented in November 2022.\n",
            "\n",
            "Response: ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It gained a million+ users within days and 100 million users within two months. Microsoft also announced a $10 billion investment in the technology.\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"ChatGPT number of tokens in context\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. However, GPT-4 itself says its context window is still 4,096 tokens. In my experience, its max completions are always around 630~820 tokens (given short prompts) and the max prompt length allowed is 3,380 tokens. def openai_chat (prompt): completions = openai.Completion.create ( engine = \"text-davinci-003\", prompt = prompt, max_tokens = 1024, n = 1, temperature = 0.8, ) response = completions.choices [0].text.strip () return response # 1. Use a single input while True: prompt = input (\"User: \") completion = openai_chat (prompt) # 2. Counting tokens for chat API calls. ChatGPT models like gpt-3.5-turbo use tokens in the same way as other models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation. Below is an example function for counting tokens for messages passed to gpt-3.5-turbo-0301. What is the technical enabler for ChatGPT to maintain the context of previous questions in its answers? For example, ChatGPT understands a prompt of \"tell me more\" and expands on it's previous answer. Does it use activations from previous questions? Is there a separate input for the context? How does it work? natural-language-processing chat-bots\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe previous search provided some information about the maximum number of tokens the ChatGPT model can keep in context, but it did not directly answer the current objective. I need to refine the search to find more specific information about the number of tokens the model can keep in context.\n",
            "\n",
            "Current objective: value='Look for information about the number of tokens the model can keep in the context.'\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"ChatGPT maximum number of tokens in context\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mdef openai_chat (prompt): completions = openai.Completion.create ( engine = \"text-davinci-003\", prompt = prompt, max_tokens = 1024, n = 1, temperature = 0.8, ) response = completions.choices [0].text.strip () return response # 1. Use a single input while True: prompt = input (\"User: \") completion = openai_chat (prompt) # 2. The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. However, GPT-4 itself says its context window is still 4,096 tokens. In my experience, its max completions are always around 630~820 tokens (given short prompts) and the max prompt length allowed is 3,380 tokens. Tokens from the prompt and the completion all together should not exceed the token limit of a particular GPT-3 model. As stated in the official OpenAI article: Depending on the model used, requests can use up to 4097 tokens shared between prompt and completion. If your prompt is 4000 tokens, your completion can be 97 tokens at most. As per the calculator, there are 46 characters in this string, which will consume 13 tokens. If you break it down into Token IDs, it looks something like this: [ 2437, 466, 314, 4727, 262, 779, 286, 16326, 287, 24101, 38, 11571, 30]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe search provided useful information about the maximum number of tokens the ChatGPT model can keep in context. I can now provide a final answer to the human's question.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"The ChatGPT model can keep a maximum of 4,096 tokens in context.\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Look for information about the number of tokens the model can keep in the context.\n",
            "\n",
            "Response: The ChatGPT model can keep a maximum of 4,096 tokens in context.\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"ChatGPT model capabilities\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe ChatGPT and GPT-4 models are language models that are optimized for conversational interfaces. The models behave differently than the older GPT-3 models. Previous models were text-in and text-out, meaning they accepted a prompt string and returned a completion to append to the prompt. last updated 15 March 2023 ChatGPT is changing fast - here's everything you need to know (Image credit: ChatGPT) Jump to: What exactly is it? What does ChatGPT stand for? When was it... Model capabilities Each model family has a series of models that are further distinguished by capability. These capabilities are typically identified by names, and the alphabetical order of these names generally signifies the relative capability and cost of that model within a given model family. We investigate the mathematical capabilities of ChatGPT by testing it on publicly available datasets, as well as hand-crafted ones, and measuring its performance against other models trained on a mathematical corpus, such as Minerva. ChatGPT Capabilities It is an advanced language model that can perform a variety of natural language processing tasks. Its main capabilities include: 1. Generate text that looks human-like It can create text that is identical to text written by humans. This is particularly useful when you need to generate content or respond to chatbot prompts. 2.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe information about the ChatGPT model capabilities has been found. I can provide the final answer now.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"The ChatGPT model is an advanced language model that can perform a variety of natural language processing tasks. Its main capabilities include generating text that looks human-like and responding to chatbot prompts.\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: If the information is not readily available, look for articles or papers that discuss the model and its capabilities.\n",
            "\n",
            "Response: The ChatGPT model is an advanced language model that can perform a variety of natural language processing tasks. Its main capabilities include generating text that looks human-like and responding to chatbot prompts.\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It can keep a maximum of 4,096 tokens in context and is capable of generating text that looks human-like and responding to chatbot prompts. Microsoft has also announced a $10 billion investment in the technology.\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Once the information is found, summarize it in a clear and concise manner.\n",
            "\n",
            "Response: ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It can keep a maximum of 4,096 tokens in context and is capable of generating text that looks human-like and responding to chatbot prompts. Microsoft has also announced a $10 billion investment in the technology.\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It can keep a maximum of 4,096 tokens in context and is capable of generating text that looks human-like and responding to chatbot prompts. Microsoft has also announced a $10 billion investment in the technology.\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Given the above steps taken, respond to the user's original question.\n",
            "\n",
            "\n",
            "\n",
            "Response: ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It can keep a maximum of 4,096 tokens in context and is capable of generating text that looks human-like and responding to chatbot prompts. Microsoft has also announced a $10 billion investment in the technology.\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It can keep a maximum of 4,096 tokens in context and is capable of generating text that looks human-like and responding to chatbot prompts. Microsoft has also announced a $10 billion investment in the technology.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask more comprehensive question"
      ],
      "metadata": {
        "id": "9Clp0VBBF6HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "planning_agent.run(\"How many request are required to summarize the book with 10000 words using OpenAI ChatGPT API?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vpxYG4A1GJOA",
        "outputId": "cf9d792b-753a-499e-bdee-8208ae1a7dbc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n",
            "steps=[Step(value='Determine the average length of a response from the OpenAI ChatGPT API.'), Step(value='Divide the total number of words in the book by the average length of a response to get the approximate number of requests needed.'), Step(value=\"Add a buffer of 10-20% to account for any missed information or errors in the API's responses.\"), Step(value='Round up to the nearest whole number to get the final number of requests needed.'), Step(value=\"Given the above steps taken, please respond to the user's original question.\\n\\n\")]\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"What is the average length of a response from the OpenAI ChatGPT API?\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThis provides lower level access than the dedicated Chat Completion API, but also requires additional input validation, only supports ChatGPT (gpt-35-turbo) models, and the underlying format is more likely to change over time. This article walks you through getting started with the new ChatGPT and GPT-4 models. ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users' feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. Try it now at chat.openai.com. Samples Please note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks. Requests with the API are taking me 50 seconds on average for some requests. Heck, a 400 token request with a 22 token response took me 15 seconds. The same exact response from ChatGPT took me 10 seconds on the website with a GPT Plus subscription, while it took me nearly 50 seconds on the API. ChatGPT gave a response that was around 650 words, and stop typing in the middle of a sentence. Is that an error on ChatGPT or a system specification limit? raymonddavey February 9, 2023, 7:12pm 4 If it stopped mid sentence it sounds like you hit your max tokens setting for the prompt and completion\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to extract the relevant information from the article and provide it as the final answer.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"What is the average length of a response from the OpenAI ChatGPT API?\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThis provides lower level access than the dedicated Chat Completion API, but also requires additional input validation, only supports ChatGPT (gpt-35-turbo) models, and the underlying format is more likely to change over time. This article walks you through getting started with the new ChatGPT and GPT-4 models. Please note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks. ChatGPT gave a response that was around 650 words, and stop typing in the middle of a sentence. Is that an error on ChatGPT or a system specification limit? raymonddavey February 9, 2023, 7:12pm 4 If it stopped mid sentence it sounds like you hit your max tokens setting for the prompt and completion Requests with the API are taking me 50 seconds on average for some requests. Heck, a 400 token request with a 22 token response took me 15 seconds. The same exact response from ChatGPT took me 10 seconds on the website with a GPT Plus subscription, while it took me nearly 50 seconds on the API. ChatGPT 3.5 api array length and prompt calculation. ChatGPT documentation Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content (the content of the message). Conversations can be as short as 1 message or fill many pages. I want to know how many arrays I can send to ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to provide the final answer to the user's question.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"The average length of a response from the OpenAI ChatGPT API can vary depending on the number of tokens used in the prompt and completion requests. Requests with the API are taking around 50 seconds on average for some requests. However, it is important to note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks.\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Determine the average length of a response from the OpenAI ChatGPT API.\n",
            "\n",
            "Response: The average length of a response from the OpenAI ChatGPT API can vary depending on the number of tokens used in the prompt and completion requests. Requests with the API are taking around 50 seconds on average for some requests. However, it is important to note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks.\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: We need to calculate the approximate number of requests needed to complete the task.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"number of words in the book / average length of a response\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-432da6599c23>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplanning_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"How many request are required to summarize the book with 10000 words using OpenAI ChatGPT API?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             outputs = (\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/experimental/plan_and_execute/agent_executor.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     46\u001b[0m             }\n\u001b[1;32m     47\u001b[0m             \u001b[0mnew_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_new_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             response = self.executor.step(\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mnew_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/experimental/plan_and_execute/executors/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, inputs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     ) -> StepResponse:\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m\"\"\"Take step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mStepResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             outputs = (\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;31m# We now enter the agent loop (until it returns something).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             next_step_output = self._take_next_step(\n\u001b[0m\u001b[1;32m    954\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    818\u001b[0m                     \u001b[0mtool_run_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"llm_prefix\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;31m# We then call the tool on the tool input to get an observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 observation = tool.run(\n\u001b[0m\u001b[1;32m    821\u001b[0m                     \u001b[0magent_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             run_manager.on_tool_end(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_args_and_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             observation = (\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtool_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtool_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/tools/base.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mnew_argument_supported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         return (\n\u001b[0;32m--> 409\u001b[0;31m             self.func(\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             outputs = (\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         )\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_llm_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_run_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     async def _acall(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py\u001b[0m in \u001b[0;36m_process_llm_result\u001b[0;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Answer: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"unknown format from LLM: {llm_output}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: unknown format from LLM: Assuming the variables \"num_words\" and \"avg_length\" represent the number of words in the book and the average length of a response, respectively:\n\n```text\nnum_words / avg_length\n```\n...numexpr.evaluate(\"num_words / avg_length\")..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "planning_agent.run(\"How many request are required to summarize the book with 10000 words using OpenAI ChatGPT API? Remember that ChatGPT API has token limit\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5GF-XJV8GULn",
        "outputId": "5adb623a-5e8b-45cf-f38f-a7e5056bb42e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n",
            "steps=[Step(value='Determine the token limit of OpenAI ChatGPT API.'), Step(value='Calculate the average number of tokens per word in the book.'), Step(value='Divide the total number of tokens in the book by the token limit to get the number of requests required.'), Step(value='Round up the result from step 3 to the nearest whole number.'), Step(value=\"Output the result from step 4 as the answer to the user's question.\\n\\n\")]\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"OpenAI ChatGPT API token limit\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. 2 Answers Sorted by: 12 As stated in the official OpenAI article: To further explore tokenization, you can use our interactive Tokenizer tool, which allows you to calculate the number of tokens and see how text is broken into tokens. Feb 6 -- 3 UPDATED: The article includes the ChatGPT API option (model=\"gpt-3.5-turbo\"). If you are reading this article, you have encountered the token limits of OpenAI's GPT-3 models. The limits for various models are provided here. https://platform.openai.com/docs/models/gpt-3 ChatGPT Token Limit Ask Question Asked 3 months ago Modified 5 days ago Viewed 14k times 13 I want ChatGPT to remember past conversations and have a consistent (stateful) conversation. I have seen several code of ChatGPT prompt engineering. There were two ways to design the prompt shown below (pseudo code): Russell Kohn · Follow 5 min read · Mar 21 -- 6 Introduction Large Language Models (LLMs) like ChatGPT have transformed the AI landscape, and understanding their intricacies is essential to...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"OpenAI ChatGPT API token limit gpt-3.5-turbo\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mChatGPT models like gpt-3.5-turbo use tokens in the same way as other models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation. Below is an example function for counting tokens for messages passed to gpt-3.5-turbo-0301. This tutorial builds on our previous video and teaches you how to handle the token limit when building a chat app based on OpenAI''s ChatGPT API (gpt-3.5-tur... UPDATED: The article includes the ChatGPT API option (model=\"gpt-3.5-turbo\"). If you are reading this article, you have encountered the token limits of OpenAI's GPT-3 models. The limits for various models are provided here. https://platform.openai.com/docs/models/gpt-3 b1narium March 31, 2023, 8:38pm 1 Hey folks, I'm currently scoping out a project for our team and i'm looking to add GPT-3.5 as a chatbot to help users better interact with our platform. The method we're using is grabbing a limited set of data (in the form of json), and letting the user ask questions to gpt 3.5 about that set. ChatGPT Token Limit Ask Question Asked 3 months ago Modified 5 days ago Viewed 14k times 13 I want ChatGPT to remember past conversations and have a consistent (stateful) conversation. I have seen several code of ChatGPT prompt engineering. There were two ways to design the prompt shown below (pseudo code):\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"The token limit for the OpenAI ChatGPT API varies depending on the model used. For example, the gpt-3.5-turbo model has a token limit, but it is difficult to count how many tokens will be used by a conversation due to its message-based formatting. It is recommended to use the Chat Completion API for interacting with the ChatGPT and GPT-4 models, as it is the preferred method for accessing these models and the only way to access the new GPT-4 models.\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Determine the token limit of OpenAI ChatGPT API.\n",
            "\n",
            "Response: The token limit for the OpenAI ChatGPT API varies depending on the model used. For example, the gpt-3.5-turbo model has a token limit, but it is difficult to count how many tokens will be used by a conversation due to its message-based formatting. It is recommended to use the Chat Completion API for interacting with the ChatGPT and GPT-4 models, as it is the preferred method for accessing these models and the only way to access the new GPT-4 models.\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I don't have a tool to directly calculate the average number of tokens per word in a book. I'll need to ask the human for more information.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"What is the average number of tokens per word in a book?\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mRoughly, one token is worth 100 words - so a 600-word chapter will require about six tokens. Easy so far. However the price of a token varies by how many you buy. The more tokens you... Episodes are between 600-5,000 words long It is currently only available to those in the U.S. Kindle Vella is available on iOS, Android and Fire devices, as well as online. Authors receive 50% of royalties Readers will spend their money on \"Tokens\" that will be used to unlock the story, episode by episode The tool of word count is available in most writing software like MS Word, Google Docs, Grammarly, Scrivener, Ulysses, etc. You can choose a target word count, and the software will be tracking your progress and tell you when you've achieved it. Since every book has a basic contract with the reader, word count is important. The ideal count for a mystery is a 70,000 and 90,000 word novel. Science fiction and fantasy: Sci-fi novels are an art in world-building. The need to invent a completely new environment makes this genre longer than others. A fantasy novel will usually have 90,000 to 120,000 words. According to a survey of over 900 authors, there is an average of 233 words per page in a book on Amazon for nonfiction or 280 for fiction novels. Font size and other factors impact this number. The standard book page for books published by traditional publishing companies ranges from 250-300 words.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to clarify with the human if they have a specific book in mind or if they are looking for a general average.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"What book are you referring to or are you looking for a general average?\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mBy Chris Drew (PhD) / October 23, 2022 Are you feeling overwhelmed by referencing? When you're first asked to do referencing in an essay it can be hard to get your head around it. If it's been a while since you were first taught how to reference, it can be intimidating to ask again how to do it! Referencing an e-book from a library database is identical to referencing a print book, there is no need to include a URL or retrieval statement. For an e-book, there are a number of ways to obtain the key information you require to create a reference. The best way is to check the details on the e-book landing page. A quick look at the 10 best ADHD books. Best classic: Driven to Distraction. Best for women: A Radical Guide for Women with ADHD. Best for adults: Thriving with Adult ADHD. Best for everyday life ... It is that simple. So, choosing your Amazon kindle categories or book categories will have a direct effect on whether or not you become a bestseller. Here's an example: If you choose a category where the #1 book in that category has an ABSR of 2,000, then you'd need to have an ABSR or 1,999 or less in order to be the new #1. Types of Reference Materials Subject Specific Reference Sources Explanation This page contains definitions of and examples of different types of reference sources including almanacs, atlases, bibliographies, biographies, chronologies, dictionaries, directories, encyclopedias, handbooks, and indexes. Almanacs\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI still need more information to calculate the average number of tokens per word in a book. I'll ask the human if they have a specific book in mind or if they are looking for a general average.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"Do you have a specific book in mind or are you looking for a general average?\"\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mA quick look at the 10 best ADHD books. Best classic: Driven to Distraction. Best for women: A Radical Guide for Women with ADHD. Best for adults: Thriving with Adult ADHD. Best for everyday life ... Share of adults who have read a book in any format in the last 12 months in the United States from 2011 to 2021, by format Premium Statistic Print book readers in the U.S. 2021, by age group For example, a searcher who would click on an in-depth B2B buyer's guide might expect an e-book over blog posts, while someone looking at just getting general overviews may prefer high-level ... 1. Tell me about yourself. What they're really asking: What makes you the right fit for this job? This question can sound broad and open ended, but it's really about your relationship with data analytics. Keep your answer focused on your journey toward becoming a data analyst. What sparked your interest in the field? Research Myths Have you ever heard people say that they are more of a right brain vs left brain thinker? From books to television programs, you've probably heard the phrase mentioned numerous times. Or perhaps you've even taken an online test to determine whether you are right-brained or left-brained.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI still don't have enough information to calculate the average number of tokens per word in a book. I'll ask the human to provide me with the word count of the book they are interested in.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"Please provide me with the word count of the book you are interested in.\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Calculate the average number of tokens per word in the book.\n",
            "\n",
            "Response: Please provide me with the word count of the book you are interested in.\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"DuckDuckGo Search\",\n",
            "  \"action_input\": \"What is the total number of tokens in the book?\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer - Total number of tokens = 14 Lexeme It is a sequence of characters in the source code that are matched by given predefined language rules for every lexeme to be specified as a valid token. 10 The NLTK book has a couple of examples of word counts, but in reality they are not word counts but token counts. For instance, Chapter 1, Counting Vocabulary says that the following gives a word count: text = nltk.Text (tokens) len (text) However, it doesn't - it gives a word and punctuation count. By running this code, we can determine the number of word tokens and word types in the book \"Austen-Persuasion.txt\". This information can be used to analyze the vocabulary richness and diversity of the book, and to compare it with other books. The term \"book token\" was first coined in the 1920s by Harold Raymond, an English publisher who noticed that for Christmas his friends had received a collective total of only three books out of 119 total gifts. [1] He felt that some sort of coupon could be used to take the risk out of giving books as gifts. Book tokens were first officially ... Compiler Design Lexical Analysis Tokens Tokens MCQ Quiz - Objective Question with Answer for Tokens - Download Free PDF Last updated on Apr 28, 2023 Latest Tokens MCQ Objective Questions Tokens Question 1: Which of the following is/are the functionality of a lexical analyzer? it recognizes lexemes and produces a stream of tokens.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to calculate the number of requests required to process the book based on the token limit. I can use the information from the previous steps to calculate this.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"14 / 4096\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 0.00341796875\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have the answer to the current objective. I can provide the final response to the human.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"The number of requests required to process the book is approximately 0.0034.\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Divide the total number of tokens in the book by the token limit to get the number of requests required.\n",
            "\n",
            "Response: The number of requests required to process the book is approximately 0.0034.\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"ceil(4096)\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4096.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to round up the result from step 3 to the nearest whole number. I can use the ceil function from the calculator tool to achieve this.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"ceil(4096)\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4096.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe result is already a whole number, so I don't need to round it up any further.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"4096\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Round up the result from step 3 to the nearest whole number.\n",
            "\n",
            "Response: 4096\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It can keep a maximum of 4,096 tokens in context and is capable of generating text that looks human-like and responding to chatbot prompts. Microsoft has also announced a $10 billion investment in the technology.\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "*****\n",
            "\n",
            "Step: Output the result from step 4 as the answer to the user's question.\n",
            "\n",
            "\n",
            "\n",
            "Response: ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It can keep a maximum of 4,096 tokens in context and is capable of generating text that looks human-like and responding to chatbot prompts. Microsoft has also announced a $10 billion investment in the technology.\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ChatGPT is an AI chatbot developed by OpenAI and released in November 2022. It can keep a maximum of 4,096 tokens in context and is capable of generating text that looks human-like and responding to chatbot prompts. Microsoft has also announced a $10 billion investment in the technology.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now it is working a little bit better, but it forgot about initial question. To improve it we can add the long-term memory to the prompt.\n"
      ],
      "metadata": {
        "id": "YvWKJZmYGPQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain.agents import Tool\n",
        "\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Define your embedding model\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "# Initialize the vectorstore as empty\n",
        "import faiss\n",
        "\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n",
        "\n",
        "\n",
        "from langchain.experimental import AutoGPT\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "tools = load_tools([\"ddg-search\", \"llm-math\"], llm=ChatOpenAI(temperature=0))\n",
        "\n",
        "agent = AutoGPT.from_llm_and_tools(\n",
        "    ai_name=\"Tom\",\n",
        "    ai_role=\"Assistant\",\n",
        "    tools=tools,\n",
        "    llm=ChatOpenAI(temperature=0),\n",
        "    memory=vectorstore.as_retriever()\n",
        ")\n",
        "# Set verbose to be true\n",
        "agent.chain.verbose = True\n",
        "\n",
        "\n",
        "agent.run([\"How many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7uzHbj0CIr2G",
        "outputId": "22292e4a-7f71-4102-886a-b5fea8981c94"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are Tom, Assistant\n",
            "Your decisions must always be made independently without seeking user assistance.\n",
            "Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
            "If you have completed all your tasks, make sure to use the \"finish\" command.\n",
            "\n",
            "GOALS:\n",
            "\n",
            "1. How many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API?\n",
            "\n",
            "\n",
            "Constraints:\n",
            "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
            "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
            "3. No user assistance\n",
            "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
            "\n",
            "Commands:\n",
            "1. DuckDuckGo Search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args json schema: {\"query\": {\"title\": \"Query\", \"type\": \"string\"}}\n",
            "2. Calculator: Useful for when you need to answer questions about math., args json schema: {\"tool_input\": {\"type\": \"string\"}}\n",
            "3. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\n",
            "\n",
            "Resources:\n",
            "1. Internet access for searches and information gathering.\n",
            "2. Long Term memory management.\n",
            "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
            "4. File output.\n",
            "\n",
            "Performance Evaluation:\n",
            "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
            "2. Constructively self-criticize your big-picture behavior constantly.\n",
            "3. Reflect on past decisions and strategies to refine your approach.\n",
            "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
            "\n",
            "You should only respond in JSON format as described below \n",
            "Response Format: \n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"thought\",\n",
            "        \"reasoning\": \"reasoning\",\n",
            "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
            "        \"criticism\": \"constructive self-criticism\",\n",
            "        \"speak\": \"thoughts summary to say to user\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"command name\",\n",
            "        \"args\": {\n",
            "            \"arg name\": \"value\"\n",
            "        }\n",
            "    }\n",
            "} \n",
            "Ensure the response can be parsed by Python json.loads\n",
            "System: The current time and date is Fri Jun  9 08:48:56 2023\n",
            "System: This reminds you of these events from your past:\n",
            "[]\n",
            "\n",
            "\n",
            "Human: Determine which next command to use, and respond using the format specified above:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"I need to determine how many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API. I will need to do some research on the API and its capabilities.\",\n",
            "        \"reasoning\": \"I need to understand the capabilities of the API before I can determine how many requests are required to summarize the book.\",\n",
            "        \"plan\": \"- Research the OpenAI ChatGPT API\\n- Determine its capabilities\\n- Use that information to determine how many requests are required to summarize the book\",\n",
            "        \"criticism\": \"I need to make sure I am efficient in my research and not waste time on irrelevant information.\",\n",
            "        \"speak\": \"I need to research the OpenAI ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"DuckDuckGo Search\",\n",
            "        \"args\": {\n",
            "            \"query\": \"OpenAI ChatGPT API capabilities\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are Tom, Assistant\n",
            "Your decisions must always be made independently without seeking user assistance.\n",
            "Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
            "If you have completed all your tasks, make sure to use the \"finish\" command.\n",
            "\n",
            "GOALS:\n",
            "\n",
            "1. How many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API?\n",
            "\n",
            "\n",
            "Constraints:\n",
            "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
            "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
            "3. No user assistance\n",
            "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
            "\n",
            "Commands:\n",
            "1. DuckDuckGo Search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args json schema: {\"query\": {\"title\": \"Query\", \"type\": \"string\"}}\n",
            "2. Calculator: Useful for when you need to answer questions about math., args json schema: {\"tool_input\": {\"type\": \"string\"}}\n",
            "3. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\n",
            "\n",
            "Resources:\n",
            "1. Internet access for searches and information gathering.\n",
            "2. Long Term memory management.\n",
            "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
            "4. File output.\n",
            "\n",
            "Performance Evaluation:\n",
            "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
            "2. Constructively self-criticize your big-picture behavior constantly.\n",
            "3. Reflect on past decisions and strategies to refine your approach.\n",
            "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
            "\n",
            "You should only respond in JSON format as described below \n",
            "Response Format: \n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"thought\",\n",
            "        \"reasoning\": \"reasoning\",\n",
            "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
            "        \"criticism\": \"constructive self-criticism\",\n",
            "        \"speak\": \"thoughts summary to say to user\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"command name\",\n",
            "        \"args\": {\n",
            "            \"arg name\": \"value\"\n",
            "        }\n",
            "    }\n",
            "} \n",
            "Ensure the response can be parsed by Python json.loads\n",
            "System: The current time and date is Fri Jun  9 08:49:09 2023\n",
            "System: This reminds you of these events from your past:\n",
            "['Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"I need to determine how many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API. I will need to do some research on the API and its capabilities.\",\\n        \"reasoning\": \"I need to understand the capabilities of the API before I can determine how many requests are required to summarize the book.\",\\n        \"plan\": \"- Research the OpenAI ChatGPT API\\\\n- Determine its capabilities\\\\n- Use that information to determine how many requests are required to summarize the book\",\\n        \"criticism\": \"I need to make sure I am efficient in my research and not waste time on irrelevant information.\",\\n        \"speak\": \"I need to research the OpenAI ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\\n    },\\n    \"command\": {\\n        \"name\": \"DuckDuckGo Search\",\\n        \"args\": {\\n            \"query\": \"OpenAI ChatGPT API capabilities\"\\n        }\\n    }\\n} \\nResult: Command DuckDuckGo Search returned: ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users\\' feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. Try it now at chat.openai.com. Samples ChatGPT and Whisper models are now available on our API, giving developers access to cutting-edge language (not just chat!) and speech-to-text capabilities. Through a series of system-wide optimizations, we\\'ve achieved 90% cost reduction for ChatGPT since December; we\\'re now passing through those savings to API users. Please note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks. Microsoft Learn. Today, we are thrilled to announce that ChatGPT is available in preview in Azure OpenAI Service. With Azure OpenAI Service, over 1,000 customers are applying the most advanced AI models—including Dall-E 2, GPT-3.5, Codex, and other large language models backed by the unique supercomputing and enterprise capabilities of Azure ... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. ']\n",
            "\n",
            "\n",
            "Human: Determine which next command to use, and respond using the format specified above:\n",
            "AI: {\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"I need to determine how many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API. I will need to do some research on the API and its capabilities.\",\n",
            "        \"reasoning\": \"I need to understand the capabilities of the API before I can determine how many requests are required to summarize the book.\",\n",
            "        \"plan\": \"- Research the OpenAI ChatGPT API\\n- Determine its capabilities\\n- Use that information to determine how many requests are required to summarize the book\",\n",
            "        \"criticism\": \"I need to make sure I am efficient in my research and not waste time on irrelevant information.\",\n",
            "        \"speak\": \"I need to research the OpenAI ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"DuckDuckGo Search\",\n",
            "        \"args\": {\n",
            "            \"query\": \"OpenAI ChatGPT API capabilities\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "System: Command DuckDuckGo Search returned: ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users' feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. Try it now at chat.openai.com. Samples ChatGPT and Whisper models are now available on our API, giving developers access to cutting-edge language (not just chat!) and speech-to-text capabilities. Through a series of system-wide optimizations, we've achieved 90% cost reduction for ChatGPT since December; we're now passing through those savings to API users. Please note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks. Microsoft Learn. Today, we are thrilled to announce that ChatGPT is available in preview in Azure OpenAI Service. With Azure OpenAI Service, over 1,000 customers are applying the most advanced AI models—including Dall-E 2, GPT-3.5, Codex, and other large language models backed by the unique supercomputing and enterprise capabilities of Azure ... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models.\n",
            "Human: Determine which next command to use, and respond using the format specified above:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"Based on the information I found, it seems that the ChatGPT API is the best option for summarizing the book. I will need to determine how many requests are required to summarize the book using this API.\",\n",
            "        \"reasoning\": \"The ChatGPT API is the best option for summarizing the book based on the information I found. I need to determine how many requests are required to summarize the book using this API.\",\n",
            "        \"plan\": \"- Determine the maximum number of words that can be summarized in a single request using the ChatGPT API\\n- Divide the total number of words in the book by the maximum number of words that can be summarized in a single request to determine how many requests are required to summarize the book\",\n",
            "        \"criticism\": \"I need to make sure I am accurate in my calculations and not make any mistakes.\",\n",
            "        \"speak\": \"I need to determine how many requests are required to summarize the book of 10000 words using the ChatGPT API.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"DuckDuckGo Search\",\n",
            "        \"args\": {\n",
            "            \"query\": \"maximum number of words that can be summarized in a single request using ChatGPT API\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are Tom, Assistant\n",
            "Your decisions must always be made independently without seeking user assistance.\n",
            "Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
            "If you have completed all your tasks, make sure to use the \"finish\" command.\n",
            "\n",
            "GOALS:\n",
            "\n",
            "1. How many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API?\n",
            "\n",
            "\n",
            "Constraints:\n",
            "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
            "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
            "3. No user assistance\n",
            "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
            "\n",
            "Commands:\n",
            "1. DuckDuckGo Search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args json schema: {\"query\": {\"title\": \"Query\", \"type\": \"string\"}}\n",
            "2. Calculator: Useful for when you need to answer questions about math., args json schema: {\"tool_input\": {\"type\": \"string\"}}\n",
            "3. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\n",
            "\n",
            "Resources:\n",
            "1. Internet access for searches and information gathering.\n",
            "2. Long Term memory management.\n",
            "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
            "4. File output.\n",
            "\n",
            "Performance Evaluation:\n",
            "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
            "2. Constructively self-criticize your big-picture behavior constantly.\n",
            "3. Reflect on past decisions and strategies to refine your approach.\n",
            "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
            "\n",
            "You should only respond in JSON format as described below \n",
            "Response Format: \n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"thought\",\n",
            "        \"reasoning\": \"reasoning\",\n",
            "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
            "        \"criticism\": \"constructive self-criticism\",\n",
            "        \"speak\": \"thoughts summary to say to user\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"command name\",\n",
            "        \"args\": {\n",
            "            \"arg name\": \"value\"\n",
            "        }\n",
            "    }\n",
            "} \n",
            "Ensure the response can be parsed by Python json.loads\n",
            "System: The current time and date is Fri Jun  9 08:49:25 2023\n",
            "System: This reminds you of these events from your past:\n",
            "['Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"I need to determine how many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API. I will need to do some research on the API and its capabilities.\",\\n        \"reasoning\": \"I need to understand the capabilities of the API before I can determine how many requests are required to summarize the book.\",\\n        \"plan\": \"- Research the OpenAI ChatGPT API\\\\n- Determine its capabilities\\\\n- Use that information to determine how many requests are required to summarize the book\",\\n        \"criticism\": \"I need to make sure I am efficient in my research and not waste time on irrelevant information.\",\\n        \"speak\": \"I need to research the OpenAI ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\\n    },\\n    \"command\": {\\n        \"name\": \"DuckDuckGo Search\",\\n        \"args\": {\\n            \"query\": \"OpenAI ChatGPT API capabilities\"\\n        }\\n    }\\n} \\nResult: Command DuckDuckGo Search returned: ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users\\' feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. Try it now at chat.openai.com. Samples ChatGPT and Whisper models are now available on our API, giving developers access to cutting-edge language (not just chat!) and speech-to-text capabilities. Through a series of system-wide optimizations, we\\'ve achieved 90% cost reduction for ChatGPT since December; we\\'re now passing through those savings to API users. Please note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks. Microsoft Learn. Today, we are thrilled to announce that ChatGPT is available in preview in Azure OpenAI Service. With Azure OpenAI Service, over 1,000 customers are applying the most advanced AI models—including Dall-E 2, GPT-3.5, Codex, and other large language models backed by the unique supercomputing and enterprise capabilities of Azure ... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. ', 'Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"Based on the information I found, it seems that the ChatGPT API is the best option for summarizing the book. I will need to determine how many requests are required to summarize the book using this API.\",\\n        \"reasoning\": \"The ChatGPT API is the best option for summarizing the book based on the information I found. I need to determine how many requests are required to summarize the book using this API.\",\\n        \"plan\": \"- Determine the maximum number of words that can be summarized in a single request using the ChatGPT API\\\\n- Divide the total number of words in the book by the maximum number of words that can be summarized in a single request to determine how many requests are required to summarize the book\",\\n        \"criticism\": \"I need to make sure I am accurate in my calculations and not make any mistakes.\",\\n        \"speak\": \"I need to determine how many requests are required to summarize the book of 10000 words using the ChatGPT API.\"\\n    },\\n    \"command\": {\\n        \"name\": \"DuckDuckGo Search\",\\n        \"args\": {\\n            \"query\": \"maximum number of words that can be summarized in a single request using ChatGPT API\"\\n        }\\n    }\\n} \\nResult: Command DuckDuckGo Search returned: 1. Find your article, paper, or book to summarize If you need ChatGPT to help summarize an article or research paper, find it online and keep it open in a separate tab. Also: This AI chatbot... This happens at about 500 words or 4,000 characters. If you happen to give the chatbot a request for a specific number of words above 500, you might find that it cuts off midsentence... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. ChatGPT-3.5 & ChatGPT-4 accept 2,500-3,000 words & 18,000-19,000 characters (prompt & response combo) ChatGPT-3.5 & ChatGPT-4 accept ~6,000-7,000 tokens (prompt & response combo) These limits may be subject to future change, so do not assume that the results from tests I did in April 2023 will apply to ChatGPT in several months or a year from now. OP perhaps you could attempt to implement whatever it is that the web ChatGPT is doing - split all your text into chunks of about 3000 words, get a summary of each in separate API calls, and then send all the summaries in another API call, to get a \"summary of summaries\" - jameslol Mar 8 at 4:43 ']\n",
            "\n",
            "\n",
            "Human: Determine which next command to use, and respond using the format specified above:\n",
            "AI: {\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"I need to determine how many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API. I will need to do some research on the API and its capabilities.\",\n",
            "        \"reasoning\": \"I need to understand the capabilities of the API before I can determine how many requests are required to summarize the book.\",\n",
            "        \"plan\": \"- Research the OpenAI ChatGPT API\\n- Determine its capabilities\\n- Use that information to determine how many requests are required to summarize the book\",\n",
            "        \"criticism\": \"I need to make sure I am efficient in my research and not waste time on irrelevant information.\",\n",
            "        \"speak\": \"I need to research the OpenAI ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"DuckDuckGo Search\",\n",
            "        \"args\": {\n",
            "            \"query\": \"OpenAI ChatGPT API capabilities\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "System: Command DuckDuckGo Search returned: ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users' feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. Try it now at chat.openai.com. Samples ChatGPT and Whisper models are now available on our API, giving developers access to cutting-edge language (not just chat!) and speech-to-text capabilities. Through a series of system-wide optimizations, we've achieved 90% cost reduction for ChatGPT since December; we're now passing through those savings to API users. Please note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks. Microsoft Learn. Today, we are thrilled to announce that ChatGPT is available in preview in Azure OpenAI Service. With Azure OpenAI Service, over 1,000 customers are applying the most advanced AI models—including Dall-E 2, GPT-3.5, Codex, and other large language models backed by the unique supercomputing and enterprise capabilities of Azure ... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models.\n",
            "Human: Determine which next command to use, and respond using the format specified above:\n",
            "AI: {\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"Based on the information I found, it seems that the ChatGPT API is the best option for summarizing the book. I will need to determine how many requests are required to summarize the book using this API.\",\n",
            "        \"reasoning\": \"The ChatGPT API is the best option for summarizing the book based on the information I found. I need to determine how many requests are required to summarize the book using this API.\",\n",
            "        \"plan\": \"- Determine the maximum number of words that can be summarized in a single request using the ChatGPT API\\n- Divide the total number of words in the book by the maximum number of words that can be summarized in a single request to determine how many requests are required to summarize the book\",\n",
            "        \"criticism\": \"I need to make sure I am accurate in my calculations and not make any mistakes.\",\n",
            "        \"speak\": \"I need to determine how many requests are required to summarize the book of 10000 words using the ChatGPT API.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"DuckDuckGo Search\",\n",
            "        \"args\": {\n",
            "            \"query\": \"maximum number of words that can be summarized in a single request using ChatGPT API\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "System: Command DuckDuckGo Search returned: 1. Find your article, paper, or book to summarize If you need ChatGPT to help summarize an article or research paper, find it online and keep it open in a separate tab. Also: This AI chatbot... This happens at about 500 words or 4,000 characters. If you happen to give the chatbot a request for a specific number of words above 500, you might find that it cuts off midsentence... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. ChatGPT-3.5 & ChatGPT-4 accept 2,500-3,000 words & 18,000-19,000 characters (prompt & response combo) ChatGPT-3.5 & ChatGPT-4 accept ~6,000-7,000 tokens (prompt & response combo) These limits may be subject to future change, so do not assume that the results from tests I did in April 2023 will apply to ChatGPT in several months or a year from now. OP perhaps you could attempt to implement whatever it is that the web ChatGPT is doing - split all your text into chunks of about 3000 words, get a summary of each in separate API calls, and then send all the summaries in another API call, to get a \"summary of summaries\" - jameslol Mar 8 at 4:43\n",
            "Human: Determine which next command to use, and respond using the format specified above:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"Based on the information I found, it seems that the maximum number of words that can be summarized in a single request using ChatGPT API is 3000 words. I can use this information to determine how many requests are required to summarize the book.\",\n",
            "        \"reasoning\": \"I found that the maximum number of words that can be summarized in a single request using ChatGPT API is 3000 words. I can use this information to determine how many requests are required to summarize the book.\",\n",
            "        \"plan\": \"- Divide the total number of words in the book by 3000 to determine how many requests are required to summarize the book\",\n",
            "        \"criticism\": \"I need to make sure I am accurate in my calculations and not make any mistakes.\",\n",
            "        \"speak\": \"I can use the maximum number of words that can be summarized in a single request using ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"Calculator\",\n",
            "        \"args\": {\n",
            "            \"tool_input\": \"10000/3000\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are Tom, Assistant\n",
            "Your decisions must always be made independently without seeking user assistance.\n",
            "Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
            "If you have completed all your tasks, make sure to use the \"finish\" command.\n",
            "\n",
            "GOALS:\n",
            "\n",
            "1. How many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API?\n",
            "\n",
            "\n",
            "Constraints:\n",
            "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
            "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
            "3. No user assistance\n",
            "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
            "\n",
            "Commands:\n",
            "1. DuckDuckGo Search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args json schema: {\"query\": {\"title\": \"Query\", \"type\": \"string\"}}\n",
            "2. Calculator: Useful for when you need to answer questions about math., args json schema: {\"tool_input\": {\"type\": \"string\"}}\n",
            "3. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\n",
            "\n",
            "Resources:\n",
            "1. Internet access for searches and information gathering.\n",
            "2. Long Term memory management.\n",
            "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
            "4. File output.\n",
            "\n",
            "Performance Evaluation:\n",
            "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
            "2. Constructively self-criticize your big-picture behavior constantly.\n",
            "3. Reflect on past decisions and strategies to refine your approach.\n",
            "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
            "\n",
            "You should only respond in JSON format as described below \n",
            "Response Format: \n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"thought\",\n",
            "        \"reasoning\": \"reasoning\",\n",
            "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
            "        \"criticism\": \"constructive self-criticism\",\n",
            "        \"speak\": \"thoughts summary to say to user\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"command name\",\n",
            "        \"args\": {\n",
            "            \"arg name\": \"value\"\n",
            "        }\n",
            "    }\n",
            "} \n",
            "Ensure the response can be parsed by Python json.loads\n",
            "System: The current time and date is Fri Jun  9 08:49:41 2023\n",
            "System: This reminds you of these events from your past:\n",
            "['Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"I need to determine how many requests are required to summarize the book of 10000 words using OpenAI ChatGPT API. I will need to do some research on the API and its capabilities.\",\\n        \"reasoning\": \"I need to understand the capabilities of the API before I can determine how many requests are required to summarize the book.\",\\n        \"plan\": \"- Research the OpenAI ChatGPT API\\\\n- Determine its capabilities\\\\n- Use that information to determine how many requests are required to summarize the book\",\\n        \"criticism\": \"I need to make sure I am efficient in my research and not waste time on irrelevant information.\",\\n        \"speak\": \"I need to research the OpenAI ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\\n    },\\n    \"command\": {\\n        \"name\": \"DuckDuckGo Search\",\\n        \"args\": {\\n            \"query\": \"OpenAI ChatGPT API capabilities\"\\n        }\\n    }\\n} \\nResult: Command DuckDuckGo Search returned: ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users\\' feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. Try it now at chat.openai.com. Samples ChatGPT and Whisper models are now available on our API, giving developers access to cutting-edge language (not just chat!) and speech-to-text capabilities. Through a series of system-wide optimizations, we\\'ve achieved 90% cost reduction for ChatGPT since December; we\\'re now passing through those savings to API users. Please note that the ChatGPT API is a general term that refers to OpenAI APIs that use GPT-based models for developing chatbots, including the gpt-3.5-turbo and gpt-4 models. The ChatGPT API is primarily optimized for chat but it also works well for text completion tasks. Microsoft Learn. Today, we are thrilled to announce that ChatGPT is available in preview in Azure OpenAI Service. With Azure OpenAI Service, over 1,000 customers are applying the most advanced AI models—including Dall-E 2, GPT-3.5, Codex, and other large language models backed by the unique supercomputing and enterprise capabilities of Azure ... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. ', 'Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"Based on the information I found, it seems that the ChatGPT API is the best option for summarizing the book. I will need to determine how many requests are required to summarize the book using this API.\",\\n        \"reasoning\": \"The ChatGPT API is the best option for summarizing the book based on the information I found. I need to determine how many requests are required to summarize the book using this API.\",\\n        \"plan\": \"- Determine the maximum number of words that can be summarized in a single request using the ChatGPT API\\\\n- Divide the total number of words in the book by the maximum number of words that can be summarized in a single request to determine how many requests are required to summarize the book\",\\n        \"criticism\": \"I need to make sure I am accurate in my calculations and not make any mistakes.\",\\n        \"speak\": \"I need to determine how many requests are required to summarize the book of 10000 words using the ChatGPT API.\"\\n    },\\n    \"command\": {\\n        \"name\": \"DuckDuckGo Search\",\\n        \"args\": {\\n            \"query\": \"maximum number of words that can be summarized in a single request using ChatGPT API\"\\n        }\\n    }\\n} \\nResult: Command DuckDuckGo Search returned: 1. Find your article, paper, or book to summarize If you need ChatGPT to help summarize an article or research paper, find it online and keep it open in a separate tab. Also: This AI chatbot... This happens at about 500 words or 4,000 characters. If you happen to give the chatbot a request for a specific number of words above 500, you might find that it cuts off midsentence... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. ChatGPT-3.5 & ChatGPT-4 accept 2,500-3,000 words & 18,000-19,000 characters (prompt & response combo) ChatGPT-3.5 & ChatGPT-4 accept ~6,000-7,000 tokens (prompt & response combo) These limits may be subject to future change, so do not assume that the results from tests I did in April 2023 will apply to ChatGPT in several months or a year from now. OP perhaps you could attempt to implement whatever it is that the web ChatGPT is doing - split all your text into chunks of about 3000 words, get a summary of each in separate API calls, and then send all the summaries in another API call, to get a \"summary of summaries\" - jameslol Mar 8 at 4:43 ', 'Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"Based on the information I found, it seems that the maximum number of words that can be summarized in a single request using ChatGPT API is 3000 words. I can use this information to determine how many requests are required to summarize the book.\",\\n        \"reasoning\": \"I found that the maximum number of words that can be summarized in a single request using ChatGPT API is 3000 words. I can use this information to determine how many requests are required to summarize the book.\",\\n        \"plan\": \"- Divide the total number of words in the book by 3000 to determine how many requests are required to summarize the book\",\\n        \"criticism\": \"I need to make sure I am accurate in my calculations and not make any mistakes.\",\\n        \"speak\": \"I can use the maximum number of words that can be summarized in a single request using ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\\n    },\\n    \"command\": {\\n        \"name\": \"Calculator\",\\n        \"args\": {\\n            \"tool_input\": \"10000/3000\"\\n        }\\n    }\\n} \\nResult: Command Calculator returned: Answer: 3.3333333333333335 ']\n",
            "\n",
            "\n",
            "Human: Determine which next command to use, and respond using the format specified above:\n",
            "AI: {\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"Based on the information I found, it seems that the ChatGPT API is the best option for summarizing the book. I will need to determine how many requests are required to summarize the book using this API.\",\n",
            "        \"reasoning\": \"The ChatGPT API is the best option for summarizing the book based on the information I found. I need to determine how many requests are required to summarize the book using this API.\",\n",
            "        \"plan\": \"- Determine the maximum number of words that can be summarized in a single request using the ChatGPT API\\n- Divide the total number of words in the book by the maximum number of words that can be summarized in a single request to determine how many requests are required to summarize the book\",\n",
            "        \"criticism\": \"I need to make sure I am accurate in my calculations and not make any mistakes.\",\n",
            "        \"speak\": \"I need to determine how many requests are required to summarize the book of 10000 words using the ChatGPT API.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"DuckDuckGo Search\",\n",
            "        \"args\": {\n",
            "            \"query\": \"maximum number of words that can be summarized in a single request using ChatGPT API\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "System: Command DuckDuckGo Search returned: 1. Find your article, paper, or book to summarize If you need ChatGPT to help summarize an article or research paper, find it online and keep it open in a separate tab. Also: This AI chatbot... This happens at about 500 words or 4,000 characters. If you happen to give the chatbot a request for a specific number of words above 500, you might find that it cuts off midsentence... The Chat Completion API is a new dedicated API for interacting with the ChatGPT and GPT-4 models. This API is the preferred method for accessing these models. It is also the only way to access the new GPT-4 models. ChatGPT-3.5 & ChatGPT-4 accept 2,500-3,000 words & 18,000-19,000 characters (prompt & response combo) ChatGPT-3.5 & ChatGPT-4 accept ~6,000-7,000 tokens (prompt & response combo) These limits may be subject to future change, so do not assume that the results from tests I did in April 2023 will apply to ChatGPT in several months or a year from now. OP perhaps you could attempt to implement whatever it is that the web ChatGPT is doing - split all your text into chunks of about 3000 words, get a summary of each in separate API calls, and then send all the summaries in another API call, to get a \"summary of summaries\" - jameslol Mar 8 at 4:43\n",
            "Human: Determine which next command to use, and respond using the format specified above:\n",
            "AI: {\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"Based on the information I found, it seems that the maximum number of words that can be summarized in a single request using ChatGPT API is 3000 words. I can use this information to determine how many requests are required to summarize the book.\",\n",
            "        \"reasoning\": \"I found that the maximum number of words that can be summarized in a single request using ChatGPT API is 3000 words. I can use this information to determine how many requests are required to summarize the book.\",\n",
            "        \"plan\": \"- Divide the total number of words in the book by 3000 to determine how many requests are required to summarize the book\",\n",
            "        \"criticism\": \"I need to make sure I am accurate in my calculations and not make any mistakes.\",\n",
            "        \"speak\": \"I can use the maximum number of words that can be summarized in a single request using ChatGPT API to determine how many requests are required to summarize the book of 10000 words.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"Calculator\",\n",
            "        \"args\": {\n",
            "            \"tool_input\": \"10000/3000\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "System: Command Calculator returned: Answer: 3.3333333333333335\n",
            "Human: Determine which next command to use, and respond using the format specified above:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"Based on my calculations, I will need 4 requests to summarize the book using the ChatGPT API. I will now finish my tasks.\",\n",
            "        \"reasoning\": \"I divided the total number of words in the book by the maximum number of words that can be summarized in a single request using ChatGPT API to determine how many requests are required to summarize the book. Based on my calculations, I will need 4 requests to summarize the book using the ChatGPT API.\",\n",
            "        \"plan\": \"- Finish my tasks\",\n",
            "        \"criticism\": \"I need to make sure I have completed all my tasks and have not missed anything.\",\n",
            "        \"speak\": \"I will need 4 requests to summarize the book of 10000 words using the ChatGPT API. I have completed all my tasks.\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"finish\",\n",
            "        \"args\": {\n",
            "            \"response\": \"I have completed all my objectives.\"\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have completed all my objectives.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it answers properly, but forgot to round the answer"
      ],
      "metadata": {
        "id": "qmTSi1kjSpXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AutoGPT pipeline has the user- and tools-memory and self-criticism. We can further extend this idea creating different roles that provides critiques to our agent (e.g. [CAMEL](https://python.langchain.com/en/latest/use_cases/agent_simulations/camel_role_playing.html)). On the other hand, we can use this pipeline to simulate the behavior of multiple agents with predefined roles (e.g. [Generative Agents](https://python.langchain.com/en/latest/use_cases/agent_simulations/characters.html))"
      ],
      "metadata": {
        "id": "Q9-LBlPrUdZY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sr_GLr0eYFSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}